{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549b6b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: requests in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (4.14.3)\n",
      "Collecting lxml\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/d7/0cdfb6c3e30893463fb3d1e52bc5f5f99684a03c29a0b6b605cfae879cd5/lxml-6.0.2-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "     ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.8/4.0 MB 1.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 1.0/4.0 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 1.0/4.0 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 1.8/4.0 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.1/4.0 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 2.6/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 2.9/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.1/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.7/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.7/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.7/4.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.0/4.0 MB 1.4 MB/s  0:00:02\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\programs\\miniconda3\\envs\\astudy\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c445a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d154cb3f",
   "metadata": {},
   "source": [
    "# 发送简单get请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e669bda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态码： 200\n",
      "请求URL： http://www.baidu.com/\n",
      "响应头部： text/html\n",
      "网页源码前500字符： <!DOCTYPE html>\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_\n"
     ]
    }
   ],
   "source": [
    "# 发送GET请求获取百度首页\n",
    "response = requests.get('http://www.baidu.com')\n",
    "\n",
    "# 查看响应基本信息\n",
    "print(\"状态码：\", response.status_code)  # 200表示请求成功\n",
    "print(\"请求URL：\", response.url)\n",
    "print(\"响应头部：\", response.headers['Content-Type'])\n",
    "\n",
    "# 查看网页源码（指定编码避免乱码）\n",
    "response.encoding = 'utf-8'\n",
    "print(\"网页源码前500字符：\", response.text[:500])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c799818d",
   "metadata": {},
   "source": [
    "带参数的 GET 请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79cc124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方式1响应： {'args': {'age': '20', 'name': 'test'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.5', 'X-Amzn-Trace-Id': 'Root=1-694f8635-083d47d424d9b38d054f8ee5'}, 'origin': '220.200.127.243', 'url': 'http://httpbin.org/get?name=test&age=20'}\n",
      "方式2响应： {'args': {'age': '30', 'name': 'python'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.5', 'X-Amzn-Trace-Id': 'Root=1-694f8636-5b39c1a444d859ea45f066ef'}, 'origin': '220.200.127.243', 'url': 'http://httpbin.org/get?name=python&age=30'}\n"
     ]
    }
   ],
   "source": [
    "# 方式1：直接在URL中拼接参数\n",
    "url1 = 'http://httpbin.org/get?name=test&age=20'\n",
    "response1 = requests.get(url1)\n",
    "print(\"方式1响应：\", response1.json())\n",
    "\n",
    "# 方式2：通过params参数传递\n",
    "params = {'name': 'python', 'age': 30}\n",
    "response2 = requests.get('http://httpbin.org/get', params=params)\n",
    "print(\"方式2响应：\", response2.json())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "359c5793",
   "metadata": {},
   "source": [
    "设置请求头与代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de252824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "带请求头的响应： {'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36', 'X-Amzn-Trace-Id': 'Root=1-694f863e-0340944a52a6db2d39f97e7a'}}\n",
      "代理使用失败： HTTPConnectionPool(host='120.25.253.234', port=8123): Max retries exceeded with url: http://httpbin.org/ip (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<HTTPConnection(host='120.25.253.234', port=8123) at 0x23fc1c01fd0>, 'Connection to 120.25.253.234 timed out. (connect timeout=5)')))\n"
     ]
    }
   ],
   "source": [
    "# 设置User-Agent模拟浏览器\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 发送带请求头的请求\n",
    "response = requests.get('http://httpbin.org/headers', headers=headers)\n",
    "print(\"带请求头的响应：\", response.json())\n",
    "\n",
    "# （可选）使用代理（注意：免费代理可能失效，需替换为有效代理）\n",
    "proxies = {\n",
    "    'http': '120.25.253.234:8123',\n",
    "    'https': '163.125.222.244:8123'\n",
    "}\n",
    "try:\n",
    "    response_proxy = requests.get('http://httpbin.org/ip', proxies=proxies, timeout=5)\n",
    "    print(\"代理IP响应：\", response_proxy.json())\n",
    "except Exception as e:\n",
    "    print(\"代理使用失败：\", str(e))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c64ca53f",
   "metadata": {},
   "source": [
    "# 发送 POST 请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef64c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POST响应： {'args': {}, 'data': '', 'files': {}, 'form': {'password': '123456', 'username': 'test_user'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '34', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36', 'X-Amzn-Trace-Id': 'Root=1-694f8647-17ef23f676d7a6972f944517'}, 'json': None, 'origin': '220.200.127.243', 'url': 'http://httpbin.org/post'}\n"
     ]
    }
   ],
   "source": [
    "# 准备POST数据\n",
    "data = {'username': 'test_user', 'password': '123456'}\n",
    "\n",
    "# 发送POST请求\n",
    "response = requests.post('http://httpbin.org/post', data=data, headers=headers)\n",
    "print(\"POST响应：\", response.json())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d43cf308",
   "metadata": {},
   "source": [
    "保存二进制文件（图片为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a3b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已保存为 scraped_image.jpg\n"
     ]
    }
   ],
   "source": [
    "# 爬取并保存图片\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'\n",
    "    }\n",
    "img_url = 'https://c-ssl.duitang.com/uploads/item/201806/19/20180619072722_csudc.png'\n",
    "img_response = requests.get(img_url, headers=headers)\n",
    "\n",
    "# 以二进制方式保存\n",
    "with open('scraped_image.jpg', 'wb') as f:\n",
    "    f.write(img_response.content)\n",
    "print(\"图片已保存为 scraped_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc03c813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200\n",
      "页面已保存为 page.html，建议用浏览器打开检查 DOM。\n",
      "网页标题： bilibili娘 - 萌娘百科 万物皆可萌的百科全书\n",
      "正文首段： \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36'\n",
    "}\n",
    "test_url = 'https://zh.moegirl.org.cn/Bilibili%E5%A8%98'\n",
    "\n",
    "resp = requests.get(test_url, headers=headers, timeout=15)\n",
    "print(\"HTTP\", resp.status_code)\n",
    "resp.encoding = resp.apparent_encoding\n",
    "\n",
    "# 可选：把HTML写到本地便于查看\n",
    "with open(\"page.html\", \"w\", encoding=resp.encoding) as f:\n",
    "    f.write(resp.text)\n",
    "print(\"页面已保存为 page.html，建议用浏览器打开检查 DOM。\")\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "# 安全读取 title\n",
    "title_tag = soup.find('title')\n",
    "if title_tag is not None:\n",
    "    print(\"网页标题：\", title_tag.get_text().strip())\n",
    "else:\n",
    "    print(\"未找到 <title> 标签\")\n",
    "\n",
    "# 示例：如果你想提取文章正文的第一段，先寻找常见容器，再检查是否存在\n",
    "# （针对 moegirl 页面，正文通常在 id 为 'bodyContent' 或 'mw-content-text' 的容器内）\n",
    "content = soup.find(id='bodyContent') or soup.find(id='mw-content-text') or soup.find('article')\n",
    "if content:\n",
    "    first_p = content.find('p')\n",
    "    if first_p:\n",
    "        print(\"正文首段：\", first_p.get_text().strip()[:200])\n",
    "    else:\n",
    "        print(\"未在内容容器中找到 <p> 段落\")\n",
    "else:\n",
    "    print(\"未找到常见的内容容器（bodyContent/mw-content-text/article）——请打开 page.html 查看实际 DOM\")\n",
    "\n",
    "# 旧代码示例中的选择器适用于其它站点（例如 .thumbnails .caption a），\n",
    "# 如果需要类似链接列表，请先在 page.html 中查找合适的 class 或 id 再写 selector：\n",
    "# e.g. links = soup.select('.some-class a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b80abdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\miniconda3\\envs\\astudy\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP status: 404\n",
      "页面已保存为 page.html，请用浏览器打开查看元素结构。\n",
      "未找到 <title> 标签\n",
      "匹配到的条目数量： 0\n",
      "示例（最多10个）： []\n",
      "未找到 id='header' 的元素\n",
      "未找到 <nav> 元素\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 1) 修复语法错误：给 URL 赋值（不要留孤立字符串）\n",
    "test_url = \"http://example.webscraping.com/places/default/index\"\n",
    "\n",
    "# 2) 请求并检查状态\n",
    "resp = requests.get(test_url, headers=headers, timeout=15, verify=False)\n",
    "print(\"HTTP status:\", resp.status_code)\n",
    "resp.encoding = resp.apparent_encoding\n",
    "\n",
    "# 保存页面以便在浏览器查看 DOM（定位合适的 selector）\n",
    "with open(\"page.html\", \"w\", encoding=resp.encoding) as f:\n",
    "    f.write(resp.text)\n",
    "print(\"页面已保存为 page.html，请用浏览器打开查看元素结构。\")\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "# 3) 安全读取 title\n",
    "title_tag = soup.find(\"title\")\n",
    "if title_tag:\n",
    "    print(\"网页标题：\", title_tag.get_text().strip())\n",
    "else:\n",
    "    print(\"未找到 <title> 标签\")\n",
    "\n",
    "# 4) 示例：查找符合特定链接模式的条目（site-specific）\n",
    "#    这个写法比依赖某些 class 更通用：抓取 href 中包含 '/places/default/view/' 的所有链接文本。\n",
    "links = soup.find_all(\"a\", href=True)\n",
    "country_links = [a for a in links if \"/places/default/view/\" in a[\"href\"]]\n",
    "print(\"匹配到的条目数量：\", len(country_links))\n",
    "print(\"示例（最多10个）：\", [a.get_text().strip() for a in country_links[:10]])\n",
    "\n",
    "# 5) 防护：在访问可能为 None 的元素前检查\n",
    "header = soup.find(id=\"header\")\n",
    "if header:\n",
    "    print(\"头部内容（截取）：\", header.get_text().strip()[:200])\n",
    "else:\n",
    "    print(\"未找到 id='header' 的元素\")\n",
    "\n",
    "nav = soup.find(\"nav\")\n",
    "if nav:\n",
    "    print(\"导航栏子节点：\", [child.name for child in nav.children if getattr(child, \"name\", None)])\n",
    "else:\n",
    "    print(\"未找到 <nav> 元素\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96110c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xpath提取的国家列表（前10个）： []\n",
      "特定国家的链接： []\n",
      "前3个国家： []\n"
     ]
    }
   ],
   "source": [
    "# 使用lxml的etree解析HTML\n",
    "html = etree.HTML(response.text)\n",
    "\n",
    "# 1. 提取所有国家链接\n",
    "country_xpath = '//div[@class=\"thumbnails\"]//div[@class=\"caption\"]/a/text()'\n",
    "xpath_countries = html.xpath(country_xpath)\n",
    "print(\"Xpath提取的国家列表（前10个）：\", xpath_countries[:10])\n",
    "\n",
    "# 2. 提取带有特定属性的元素\n",
    "link_xpath = '//a[@href=\"/places/default/view/Andorra-2\"]/@href'\n",
    "specific_link = html.xpath(link_xpath)\n",
    "print(\"特定国家的链接：\", specific_link)\n",
    "\n",
    "# 3. 谓语筛选（提取前3个国家）\n",
    "top3_countries = html.xpath('//div[@class=\"thumbnails\"]//a[position()<=3]/text()')\n",
    "print(\"前3个国家：\", top3_countries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
